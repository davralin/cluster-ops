---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: flux-system
spec:
  interval: 30m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: 77.8.0
      sourceRef:
        kind: HelmRepository
        name: prometheus-community-charts
        namespace: flux-system
      interval: 30m
  targetNamespace: monitoring
  install:
    createNamespace: true
    crds: Skip
    remediation:
      retries: 10
  upgrade:
    crds: Skip
    remediation:
      retries: 10
  dependsOn:
    - name: kube-prometheus-stack-crds
      namespace: flux-system
    - name: rook-ceph-cluster
      namespace: flux-system
  values:
    fullnameOverride: "kube-prom"
    crds:
      enabled: false
    cleanPrometheusOperatorObjectNames: true
    alertmanager:
      alertmanagerSpec:
        logLevel: debug
      config:
        receivers:
          - name: "Discord"
            discord_configs:
              - webhook_url: "${SECRET_DISCORD_ALERTMANAGER_URL}"
                title: |-
                  [{{ .Status | toUpper }}:{{ if eq .Status "firing" }}{{ .Alerts.Firing | len }}{{ else }}{{ .Alerts.Resolved | len }}{{ end }}]
                message: |-
                  {{- range .Alerts }}
                    **{{ .Labels.alertname }} {{ if ne .Labels.severity "" }}({{ .Labels.severity | title }}){{ end }} **
                    {{ if ne .Labels.namespace "" }}**Namespace: ** {{ .Labels.namespace | title }} {{ end }}
                    {{- if ne .Annotations.description "" }}
                      **Description:** {{ .Annotations.description }}
                    {{- else if ne .Annotations.message "" }}
                      **Message:** {{ .Annotations.message }}
                    {{- else if ne .Annotations.summary "" }}
                      **Summary:** {{ .Annotations.summary }}
                    {{- else }}
                      **Description:** N/A
                    {{- end }}
                    {{ if ne .Annotations.runbook_url "" }}**Runbook: ** {{ .Annotations.runbook_url }} {{ end }}
                  {{- end }}
          - name: "dummy"
          - name: "healthchecksio"
            webhook_configs:
              - send_resolved: true
                url: "${SECRET_ALERTMANAGER_HEALTHCHECKS_IO}"
        route:
          group_by: ["..."]
          group_wait: 30s
          group_interval: 5m
          repeat_interval: 12h
          receiver: "Discord"
          routes:
            - receiver: "healthchecksio"
              matchers:
                - alertname =~ "Watchdog"
              continue: false
            - receiver: "dummy"
              matchers:
                - alertname =~ "InfoInhibitor"
              continue: false
              # These trigger when I run my backups, and media-jobs - creating false alerts
              # I can't override/disable spesific alerts, so i Null them and
              # create new alerts which must be firing longer before alerting.
            - receiver: "dummy"
              matchers:
                - alertname =~ "NodeDiskIOSaturation|NodeSystemSaturation"
              continue: false
            - receiver: "dummy"
              matchers: # https://github.com/davralin/cluster-ops/issues/4635
                - alertname =~ "etcdHighNumberOfLeaderChanges|etcdDatabaseHighFragmentationRatio|etcdMemberCommunicationSlow|etcdHighNumberOfFailedGRPCRequests"
              continue: false
      externalUrl: https://alertmanager.${SECRET_DEFAULT_DOMAIN}
      ingress:
        enabled: true
        pathType: Prefix
        annotations:
          haproxy.org/allow-list: "${HAPROXY_WHITELIST}"
          haproxy.org/response-set-header: |
            Strict-Transport-Security "max-age=31536000"
            X-Frame-Options "DENY"
            X-Content-Type-Options "nosniff"
            Referrer-Policy "strict-origin-when-cross-origin"
        hosts:
         - "alertmanager.${SECRET_DEFAULT_DOMAIN}"
        tls:
          - secretName: "${SECRET_DEFAULT_DOMAIN_CERT}"
    defaultRules:
      rules:
        kubelet: false
    kubeApiServer:
      enabled: false # Disabled because of kubevirt KubeClientCertificateExpiration
      serviceMonitor:
        selector:
          k8s-app: kube-apiserver
    kubeControllerManager:
      service:
        selector:
          k8s-app: kube-controller-manager
    kubeEtcd:
      service:
        selector:
          k8s-app: kube-controller-manager
    kubelet:
      enabled: true
    kube-state-metrics:
      fullnameOverride: kube-state-metrics
      metricLabelsAllowlist:
        - pods=[*]
        - deployments=[*]
        - persistentvolumeclaims=[*]
      networkPolicy:
        enabled: true
        flavor: cilium
      prometheus:
        monitor:
          enabled: true
          metrics:
            relabelings:
              - action: replace
                regex: (.*)
                replacement: $1
                sourceLabels: ["__meta_kubernetes_pod_node_name"]
                targetLabel: kubernetes_node
    kubeProxy:
      enabled: false
    kubeScheduler:
      service:
        selector:
          k8s-app: kube-scheduler
    nodeExporter:
      operatingSystems:
        aix:
          enabled: false
        darwin:
          enabled: false
    prometheus-node-exporter:
      fullnameOverride: node-exporter
      prometheus:
        monitor:
          enabled: true
          relabelings:
            - action: replace
              regex: (.*)
              replacement: $1
              sourceLabels: ["__meta_kubernetes_pod_node_name"]
              targetLabel: kubernetes_node
    grafana:
      enabled: false
      forceDeployDashboards: true
    prometheus:
      ingress:
        enabled: true
        annotations:
          haproxy.org/allow-list: "${HAPROXY_WHITELIST}"
          haproxy.org/response-set-header: |
            Strict-Transport-Security "max-age=31536000"
            X-Frame-Options "DENY"
            X-Content-Type-Options "nosniff"
            Referrer-Policy "strict-origin-when-cross-origin"
        hosts:
          - "prometheus.${SECRET_DEFAULT_DOMAIN}"
        tls:
          - secretName: "${SECRET_DEFAULT_DOMAIN_CERT}"
      prometheusSpec:
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false
        scrapeConfigSelectorNilUsesHelmValues: false
        serviceMonitorSelectorNilUsesHelmValues: false
        enableAdminAPI: true
        walCompression: true
        enableFeatures:
          - memory-snapshot-on-shutdown
        retention: 60d
        retentionSize: 80GB
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: "${STORAGE_READWRITEONCE}"
              resources:
                requests:
                  storage: 200Gi
    additionalPrometheusRulesMap:
      node-rules:
        groups:
          - name: node
            rules:
              - alert: KubernetesNodeDiskUsagePercentage
                expr: (100 - 100 * sum(node_filesystem_avail_bytes{device!~"tmpfs|by-uuid",fstype=~"xfs|ext4"} / node_filesystem_size_bytes{device!~"tmpfs|by-uuid",fstype=~"xfs|ext4"}) BY (instance,device)) > 85
                for: 5m
                labels:
                  context: node
                  severity: warning
                annotations:
                  description: Node disk usage above 85%
                  summary: Disk usage on target {{ $labels.instance }} at 85%
              - alert: ExpandedNodeDiskIOSaturation
                expr: rate(node_disk_io_time_weighted_seconds_total{job="node-exporter", device=~"(/dev/)?(mmcblk.p.+|nvme.+|rbd.+|sd.+|vd.+|xvd.+|dm-.+|md.+|dasd.+)"}[5m]) > 10
                for: 120m
                labels:
                  context: node
                  severity: warning
                annotations:
                  description: |
                    Disk IO queue (aqu-sq) is high on {{ $labels.device }} at {{ $labels.instance }}, has been above 10 for the last 30 minutes, is currently at {{ printf "%.2f" $value }}.
                    This symptom might indicate disk saturation.
                  runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodediskiosaturation
                  summary: Disk IO queue is high.
              - alert: ExpandedNodeSystemSaturation
                expr: |-
                  node_load1{job="node-exporter"} / count without (cpu, mode) (node_cpu_seconds_total{job="node-exporter", mode="idle"}) > 2
                for: 60m
                labels:
                  context: node
                  severity: warning
                annotations:
                  description: |
                    System load per core at {{ $labels.instance }} has been above 2 for the last 15 minutes, is currently at {{ printf "%.2f" $value }}.
                    This might indicate this instance resources saturation and can cause it becoming unresponsive.
                  runbook_url: https://runbooks.prometheus-operator.dev/runbooks/node/nodesystemsaturation
                  summary: System saturated, load per core is very high.
      oom-rules:
        groups:
          - name: oom
            rules:
              - alert: OomKilled
                annotations:
                  summary: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.
                expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
                labels:
                  severity: critical
      pod-rules:
        groups:
          - name: pod
            rules:
              - alert: KubernetesPodCrashLooping
                expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
                for: 2m
                annotations:
                  summary: Kubernetes pod crash looping (instance {{ $labels.instance }})
                  description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      pvc-rules:
        groups:
          - name: pvc
            rules:
              - alert: KubernetesVolumeAlmostOutOfDiskSpace
                expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
                for: 10m
                labels:
                  severity: critical
                annotations:
                  summary: Kubernetes Volume almost out of disk space (instance {{ $labels.instance }})
                  description: "Volume is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      zfs-rules:
        groups:
          - name: zfs
            rules:
              - alert: ZfsUnexpectedPoolState
                annotations:
                  summary: ZFS pool {{$labels.zpool}} on {{$labels.instance}} is in a unexpected state {{$labels.state}}
                expr: node_zfs_zpool_state{state!="online"} > 0
                labels:
                  severity: critical
