---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: kube-prometheus-stack
  namespace: flux-system
spec:
  interval: 30m
  chart:
    spec:
      chart: kube-prometheus-stack
      version: 68.2.1
      sourceRef:
        kind: HelmRepository
        name: prometheus-community-charts
        namespace: flux-system
      interval: 30m
  targetNamespace: monitoring
  install:
    createNamespace: true
    crds: Skip
    remediation:
      retries: 3
  upgrade:
    crds: Skip
    remediation:
      retries: 10
  dependsOn:
    - name: kube-prometheus-stack-crds
      namespace: flux-system
    - name: rook-ceph-cluster
      namespace: flux-system
  values:
    fullnameOverride: "kube-prom"
    crds:
      enabled: false
    cleanPrometheusOperatorObjectNames: true
    alertmanager:
      alertmanagerSpec:
        alertmanagerConfiguration:
          name: alertmanager
      externalUrl: https://alertmanager.${SECRET_DEFAULT_DOMAIN}
      ingress:
        enabled: true
        pathType: Prefix
        annotations:
          haproxy.org/allow-list: "${HAPROXY_WHITELIST}"
          haproxy.org/response-set-header: |
            Strict-Transport-Security "max-age=31536000"
            X-Frame-Options "DENY"
            X-Content-Type-Options "nosniff"
            Referrer-Policy "no-referrer-when-downgrade"
        hosts:
         - "alertmanager.${SECRET_DEFAULT_DOMAIN}"
        tls:
          - secretName: "${SECRET_DEFAULT_DOMAIN_CERT}"
    defaultRules:
      create: true
      rules:
        kubelet: false
    kubeApiServer:
      enabled: false # Disabled because of kubevirt KubeClientCertificateExpiration
    kubeControllerManager:
      enabled: true
      endpoints:
        - 10.0.1.67
        - 10.0.1.68
        - 10.0.1.69
    kubeEtcd:
      enabled: true
      endpoints:
        - 10.0.1.67
        - 10.0.1.68
        - 10.0.1.69
      service:
        enabled: true
        port: 2381
        targetPort: 2381
    kubelet:
      enabled: true
    kube-state-metrics:
      enabled: true
    kubeProxy:
      enabled: false
    kubeScheduler:
      enabled: true
      endpoints:
        - 10.0.1.67
        - 10.0.1.68
        - 10.0.1.69
    nodeExporter:
      operatingSystems:
        aix:
          enabled: false
        darwin:
          enabled: false
    prometheus-node-exporter:
      fullnameOverride: node-exporter
    grafana:
      enabled: false
      forceDeployDashboards: true
    prometheus:
      ingress:
        enabled: true
        annotations:
          haproxy.org/allow-list: "${HAPROXY_WHITELIST}"
          haproxy.org/response-set-header: |
            Strict-Transport-Security "max-age=31536000"
            X-Frame-Options "DENY"
            X-Content-Type-Options "nosniff"
            Referrer-Policy "no-referrer-when-downgrade"
        hosts:
          - "prometheus.${SECRET_DEFAULT_DOMAIN}"
        tls:
          - secretName: "${SECRET_DEFAULT_DOMAIN_CERT}"
      prometheusSpec:
        retention: 31d
        retentionSize: 40GB
        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: "${STORAGE_READWRITEONCE}"
              resources:
                requests:
                  storage: 50Gi
        scrapeConfigSelector: {}
        scrapeConfigNamespaceSelector: {}
        serviceMonitorSelectorNilUsesHelmValues: false
        podMonitorSelectorNilUsesHelmValues: false
        probeSelectorNilUsesHelmValues: false
        ruleSelectorNilUsesHelmValues: false
        serviceMonitorSelector: {}
        serviceMonitorNamespaceSelector: {}
    additionalPrometheusRulesMap:
      dockerhub-rules:
        groups:
          - name: dockerhub
            rules:
              - alert: DockerhubRateLimitRisk
                annotations:
                  summary: Kubernetes cluster Dockerhub rate limit risk
                expr: count(time() - container_last_seen{image=~"(docker.io).*",container!=""} < 30) > 100
                labels:
                  severity: critical
      node-rules:
        groups:
          - name: node
            rules:
              - alert: InstanceDown
                expr: up == 0
                for: 5m
                labels:
                  severity: critical
                annotations:
                  description: '{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes.'
                  summary: 'Instance {{ $labels.instance }} down'
              - alert: KubernetesNodeDiskUsagePercentage
                expr: (100 - 100 * sum(node_filesystem_avail_bytes{device!~"tmpfs|by-uuid",fstype=~"xfs|ext4"} / node_filesystem_size_bytes{device!~"tmpfs|by-uuid",fstype=~"xfs|ext4"}) BY (instance,device)) > 85
                for: 5m
                labels:
                  context: node
                annotations:
                  description: Node disk usage above 85%
                  summary: Disk usage on target {{ $labels.instance }} at 85%
      oom-rules:
        groups:
          - name: oom
            rules:
              - alert: OomKilled
                annotations:
                  summary: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.
                expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
                labels:
                  severity: critical
      pod-rules:
        groups:
          - name: pod
            rules:
              - alert: KubernetesPodCrashLooping
                expr: increase(kube_pod_container_status_restarts_total[1m]) > 3
                for: 2m
                annotations:
                  summary: Kubernetes pod crash looping (instance {{ $labels.instance }})
                  description: "Pod {{ $labels.pod }} is crash looping\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
              - alert: OOMKilled
                annotations:
                  summary: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.
                expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
                labels:
                  severity: critical
      pvc-rules:
        groups:
          - name: pvc
            rules:
              - alert: KubernetesVolumeAlmostOutOfDiskSpace
                expr: kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes * 100 < 10
                for: 10m
                labels:
                  severity: critical
                annotations:
                  summary: Kubernetes Volume almost out of disk space (instance {{ $labels.instance }})
                  description: "Volume is almost full (< 10% left)\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      zfs-rules:
        groups:
          - name: zfs
            rules:
              - alert: ZfsUnexpectedPoolState
                annotations:
                  summary: ZFS pool {{$labels.zpool}} on {{$labels.instance}} is in a unexpected state {{$labels.state}}
                expr: node_zfs_zpool_state{state!="online"} > 0
                labels:
                  severity: critical