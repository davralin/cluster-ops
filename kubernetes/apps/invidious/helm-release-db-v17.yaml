---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &name invidious-postgres-v17
  namespace: flux-system
spec:
  interval: 30m
  chart:
    spec:
      chart: raw
      version: v0.3.2
      sourceRef:
        kind: HelmRepository
        name: dysnix-charts
        namespace: flux-system
      interval: 30m
  targetNamespace: &namespace invidious
  install:
    createNamespace: true
    remediation:
      retries: 5
  upgrade:
    remediation:
      retries: 5
  dependsOn:
    - name: cloudnative-pg
      namespace: flux-system
  values:
    resources:
      - apiVersion: postgresql.cnpg.io/v1
        kind: Cluster
        metadata:
          name: *name
          namespace: *namespace
        spec:
          # using only 1 replica: very difficult to drain the node where postgres is running
          # using more than 1 replica: write amplification issues when leveraging replicated storage (e.g. ceph)
          instances: 2
          imageName: ghcr.io/cloudnative-pg/postgresql:17.2-9
          primaryUpdateStrategy: unsupervised
          storage:
            size: 2Gi
            storageClass: "${STORAGE_READWRITEONCE}"
          managed:
            roles:
              - name: kemal
                login: true
          superuserSecret:
            name: *name
          bootstrap:
            # use this to start a new cluster
            #initdb:
            #  database: invidious
            #  owner: kemal
            #  secret:
            #    name: invidious-basic-auth
            # use this to recover a net-new cluster from a backup
            recovery:
              source: *name
              database: invidious
              owner: kemal
            # # use this to 'migrate' from an existing cnpg cluster (e.g. "invidious-postgres-v16") to the new cluster
            # initdb:
            #   import:
            #     type: monolith
            #     databases:
            #       - "*"
            #     roles:
            #       - "*"
            #     source:
            #       externalCluster: invidious-postgres-v16
          monitoring:
            enablePodMonitor: "${MONITORING_PROMETHEUS}"
          resources:
            requests:
              memory: "128Mi"
              cpu: "200m"
            limits:
              memory: "256Mi"
          backup:
            retentionPolicy: 7d
            barmanObjectStore:
              wal:
                compression: bzip2
                maxParallel: 4
              destinationPath: s3://postgresql-${CLUSTER_NAME}
              endpointURL: "${SECRET_S3_URL}"
              serverName: *name
              s3Credentials:
                accessKeyId:
                  name: *name
                  key: s3-access-key
                secretAccessKey:
                  name: *name
                  key: s3-secret-key
          externalClusters:
            # this represents the s3 backup to restore from. Must be same version.
            - name: *name
              barmanObjectStore:
                wal:
                  compression: bzip2
                  maxParallel: 8
                destinationPath: s3://postgresql-${CLUSTER_NAME}
                serverName: *name
                endpointURL: "${SECRET_S3_URL}"
                s3Credentials:
                  accessKeyId:
                    name: *name
                    key: s3-access-key
                  secretAccessKey:
                    name: *name
                    key: s3-secret-key
            # # this represents an existing cnpg cluster to migrate from (e.g. upgrading from postgres v16 to postgres v17)
             #- name: invidious-postgres-v16
             #  connectionParameters:
             #    host: invidious-postgres-v16
             #    user: postgres
             #    dbname: postgres
             #    sslmode: require
             #  password:
             #    name: invidious-postgres-v16
             #    key: password
      - apiVersion: postgresql.cnpg.io/v1
        kind: ScheduledBackup
        metadata:
          name: *name
          namespace: *namespace
        spec:
          schedule: "0 37 01 * * *"
          immediate: true
          backupOwnerReference: self
          cluster:
            name: *name
