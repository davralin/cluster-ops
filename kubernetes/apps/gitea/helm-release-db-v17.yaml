---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &name gitea-postgres-v17
  namespace: flux-system
spec:
  interval: 30m
  chart:
    spec:
      chart: raw
      version: v0.3.2
      sourceRef:
        kind: HelmRepository
        name: dysnix-charts
        namespace: flux-system
      interval: 30m
  targetNamespace: &namespace gitea
  install:
    createNamespace: true
    remediation:
      retries: 10
  upgrade:
    remediation:
      retries: 10
  dependsOn:
    - name: cloudnative-pg
      namespace: flux-system
  values:
    resources:
      - apiVersion: postgresql.cnpg.io/v1
        kind: Cluster
        metadata:
          name: *name
          namespace: *namespace
        spec:
          # using only 1 replica: very difficult to drain the node where postgres is running
          # using more than 1 replica: write amplification issues when leveraging replicated storage (e.g. ceph)
          instances: 2
          imageName: ghcr.io/cloudnative-pg/postgresql:17.4-10
          primaryUpdateStrategy: unsupervised
          storage:
            size: 2Gi
            storageClass: "${STORAGE_READWRITEONCE}"
          managed:
            roles:
              - name: gitea
                login: true
                passwordSecret:
                  name: gitea-basic-auth
          superuserSecret:
            name: *name
          bootstrap:
            # use this to start a new cluster
            #initdb:
            #  database: gitea
            #  owner: gitea
            #  secret:
            #    name: gitea-basic-auth
            # use this to recover a net-new cluster from a backup
            recovery:
              source: *name
              database: gitea
              owner: gitea
            # # use this to 'migrate' from an existing cnpg cluster (e.g. "gitea-postgres-v16") to the new cluster
            #initdb:
            #  database: gitea
            #  owner: gitea
            #  import:
            #    type: microservice
            #    databases:
            #      - gitea
            #    source:
            #      externalCluster: gitea-postgres-v16
          monitoring:
            enablePodMonitor: "${MONITORING_PROMETHEUS}"
          resources:
            requests:
              cpu: "200m"
              memory: "128Mi"
            limits:
              memory: "1Gi"
          backup:
            retentionPolicy: 7d
            barmanObjectStore:
              wal:
                compression: bzip2
                maxParallel: 4
              destinationPath: s3://postgresql-${CLUSTER_NAME}
              endpointURL: "${SECRET_S3_URL}"
              serverName: *name
              s3Credentials:
                accessKeyId:
                  name: *name
                  key: s3-access-key
                secretAccessKey:
                  name: *name
                  key: s3-secret-key
          externalClusters:
            # this represents the s3 backup to restore from. Must be same version.
            - name: *name
              barmanObjectStore:
                wal:
                  compression: bzip2
                  maxParallel: 8
                destinationPath: s3://postgresql-${CLUSTER_NAME}
                serverName: *name
                endpointURL: "${SECRET_S3_URL}"
                s3Credentials:
                  accessKeyId:
                    name: *name
                    key: s3-access-key
                  secretAccessKey:
                    name: *name
                    key: s3-secret-key
            # # this represents an existing cnpg cluster to migrate from (e.g. upgrading from postgres v16 to postgres v17)
             #- name: gitea-postgres-v16
             #  connectionParameters:
             #    host: gitea-postgres-v16.gitea.svc.cluster.local
             #    user: gitea
             #    dbname: gitea
             #    sslmode: require
             #  password:
             #    name: gitea-basic-auth
             #    key: password
      - apiVersion: postgresql.cnpg.io/v1
        kind: ScheduledBackup
        metadata:
          name: *name
          namespace: *namespace
        spec:
          schedule: "0 37 01 * * *"
          immediate: true
          backupOwnerReference: self
          cluster:
            name: *name
