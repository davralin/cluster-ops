---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &name litellm
  namespace: flux-system
spec:
  interval: 30m
  chart:
    spec:
      chart: app-template
      version: 4.5.0
      sourceRef:
        kind: HelmRepository
        name: bjw-s-charts
        namespace: flux-system
      interval: 30m
  targetNamespace: litellm
  install:
    createNamespace: true
    remediation:
      retries: 10
  upgrade:
    remediation:
      retries: 10
  values:
    fullnameOverride: *name
    defaultPodOptions:
      automountServiceAccountToken: false
      enableServiceLinks: false
      securityContext:
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        fsGroupChangePolicy: "OnRootMismatch"
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
    controllers:
      litellm:
        strategy: RollingUpdate
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          litellm:
            image:
              repository: ghcr.io/berriai/litellm-non_root
              tag: main-v1.79.1-stable@sha256:367b3c64c1f2cfcbfac5f3f9d8a32971c8f54fdb07d76c58b7a1756a5bf16121
            env:
              TZ: "${TIMEZONE}"
              LITELLM_MASTER_KEY: "sk-1234"
            probes:
              liveness: &probes
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /health/liveliness
                    port: 4000
                  initialDelaySeconds: 5
              readiness: *probes
              startup: *probes
            args:
              - "--config"
              - "/app/config.yaml"
            securityContext:
              allowPrivilegeEscalation: false
              readOnlyRootFilesystem: true
              capabilities:
                drop:
                  - ALL
            resources:
              requests:
                cpu: 10m
                memory: 200Mi
              limits:
                memory: 600Mi
    service:
      litellm:
        controller: *name
        ports:
          http:
            port: &port 4000
    ingress:
      litellm:
        enabled: true
        annotations:
          haproxy.org/allow-list: "${HAPROXY_WHITELIST}"
          haproxy.org/ssl-redirect-port: "443"
          haproxy.org/response-set-header: |
            Strict-Transport-Security "max-age=31536000"
            X-Frame-Options "DENY"
            X-Content-Type-Options "nosniff"
            Referrer-Policy "strict-origin-when-cross-origin"
        hosts:
          - host: &host "litellm.${SECRET_DEFAULT_DOMAIN}"
            paths:
              - path: /
                service:
                  identifier: litellm
                  port: http
        tls:
          - hosts:
              - *host
    configMaps:
      config:
        enabled: true
        data:
          config.yaml: |
            model_list:
              - model_name: deepseek-r1:7b
                litellm_params:
                  model: ollama_chat/deepseek-r1:7b
                  api_base: "http://ollama.ollama.svc.cluster.local:11434"
              - model_name: gemma3:12b
                litellm_params:
                  model: ollama_chat/gemma3:12b
                  api_base: "http://ollama.ollama.svc.cluster.local:11434"
              - model_name: gpt-oss:20b
                litellm_params:
                  model: ollama_chat/gpt-oss:20b
                  api_base: "http://ollama.ollama.svc.cluster.local:11434"
              - model_name: qwen3:8b
                litellm_params:
                  model: ollama_chat/qwen3:8b
                  api_base: "http://ollama.ollama.svc.cluster.local:11434"
              - model_name: qwen3:14b
                litellm_params:
                  model: ollama_chat/qwen3:14b
                  api_base: "http://ollama.ollama.svc.cluster.local:11434"
    persistence:
      config-file:
        type: configMap
        name: litellm
        globalMounts:
          - path: /app/config.yaml
            subPath: config.yaml
            readOnly: true
    networkpolicies:
      litellm:
        enabled: true
        controller: *name
        policyTypes:
          - Egress
          - Ingress
        rules:
          egress:
            - to:
                - namespaceSelector:
                    matchLabels:
                      kubernetes.io/metadata.name: ollama
                  podSelector:
                    matchLabels:
                      app.kubernetes.io/controller: ollama
                      app.kubernetes.io/instance: ollama-ollama
                      app.kubernetes.io/name: ollama-ollama
              ports:
                - port: 11434
                  protocol: TCP
            - to:
                - namespaceSelector:
                    matchLabels:
                      kubernetes.io/metadata.name: kube-system
                  podSelector:
                    matchLabels:
                      k8s-app: kube-dns
              ports:
                - port: 53
                  protocol: UDP
          ingress:
            - from:
                - namespaceSelector:
                    matchLabels:
                      name: haproxy-controller
                  podSelector:
                    matchLabels:
                      app.kubernetes.io/name: kubernetes-ingress
                - namespaceSelector:
                    matchLabels:
                      kubernetes.io/metadata.name: openwebui
                  podSelector:
                    matchLabels:
                      app.kubernetes.io/controller: openwebui
                      app.kubernetes.io/instance: openwebui-openwebui
                      app.kubernetes.io/name: openwebui-openwebui
              ports:
                - port: *port